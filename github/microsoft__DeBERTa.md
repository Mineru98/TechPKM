---
Language: Python  
tags:  
 - 자연어 처리  
 - 트랜스포머 모델  
 - 사전 학습 언어 모델  
aliases:  
 - DeBERTa  
 - 디버타  
 - 마이크로소프트 디버타  
url: https://github.com/microsoft/DeBERTa  
---  
DeBERTa는 BERT와 RoBERTa 모델을 개선하기 위해 디센탱글드 어텐션 메커니즘과 향상된 마스크 디코더를 도입한 트랜스포머 기반 자연어 처리 모델입니다. 이 프로젝트는 Microsoft에서 개발되었으며, SuperGLUE 벤치마크에서 인간 성능과 T5 11B 모델을 초과하는 성능을 달성했습니다. 다양한 크기의 사전 학습 모델(V2, V3 버전 포함)과 다국어 지원 모델을 제공하며, GLUE, SQuAD, XNLI 등의 다운스트림 작업에서 우수한 효율성을 보입니다. Hugging Face를 통해 모델 접근 및 파인튜닝이 가능합니다.