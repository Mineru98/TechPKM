---
Language: Python
tags:
 - LLM 최적화
 - 블록 확장
 - Mistral-7B
 - MetaMath
 - LLaMA
aliases:
 - LLaMA Pro
 - 블록 확장 LLM
 - LLaMA-Pro-8B
 - MetaMath-Mistral-Pro
url: https://github.com/TencentARC/LLaMA-Pro
---
LLaMA Pro는 블록 확장 기법을 활용한 점진적 LLM 최적화 프로젝트입니다. 기존 LLaMA와 Mistral 모델을 기반으로 수학/코드 성능을 향상시킨 파생 모델을 제공하며, ACL 2024 메인 컨퍼런스에 논문이 채택되었습니다. MetaMath-Mistral-Pro 모델은 GSM8k(78.4%) 및 MATH(30.3%) 벤치마크에서 SOTA 성능을 달성했습니다. 오픈소스 저장소에는 데모, 학습 코드, 평가 방법이 포함되어 있습니다.