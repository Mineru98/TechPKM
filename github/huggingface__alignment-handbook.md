---
Language: Python  
tags:  
 - 언어모델정렬  
 - 강화학습  
 - DPO  
 - ORPO  
 - ML파이프라인  
aliases:  
 - AlignmentHandbook  
 - 언어모델튜닝가이드  
 - HuggingFace정렬핸드북  
url: https://github.com/huggingface/alignment-handbook  
---
**The Alignment Handbook**은 언어 모델을 인간 및 AI 선호도에 맞추는 포괄적인 훈련 레시피 모음입니다. 사전 훈련, 지도 미세 조정(SFT), DPO, ORPO와 같은 최신 정렬 기법을 통해 모델의 유용성과 안전성을 향상시키는 방법을 제공합니다. 이 프로젝트는 Hugging Face와 협력하여 개발된 Zephyr, SmolLM 등의 모델 훈련 사례와 데이터셋, 평가 지표를 공유하며 커뮤니티 리소스 부족 문제를 해결합니다.  

주요 기술:  
- 지속적 사전 훈련 (언어/도메인 적응)  
- 지도 미세 조정 (SFT)  
- 직접 선호도 최적화 (DPO)  
- 확률비 선호도 최적화 (ORPO)  
- 강화학습 대체 기법  

Obsidian 관리용 메타데이터 문서 표준에 따라 요약되었습니다.