---
Language: Python  
tags:  
 - 언어모델  
 - 오픈소스  
 - Llama  
 - 분산학습  
 - 에지컴퓨팅  
aliases:  
 - TinyLlama  
 - 소형언어모델  
 - 1.1B 파라미터  
url: https://github.com/jzhang38/TinyLlama  
---  
TinyLlama는 16개의 A100-40G GPU로 3조 토큰을 학습시켜 1.1B 파라미터 규모의 Llama 2 호환 언어모델을 개발하는 프로젝트입니다. 모델 경량화를 통해 제한된 컴퓨팅 자원에서도 실시간 번역, 게임 대화 생성 등에 활용 가능하며, Megatron-LM 대비 효율적인 50억 파라미터 미만 모델 사전학습 참고 구현을 제공합니다. Llama 2와 동일한 아키텍처를 채택해 기존 오픈소스 생태계와 호환됩니다.