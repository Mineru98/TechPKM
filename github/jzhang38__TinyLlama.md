---
Language: Python
tags:
 - 언어 모델
 - LLaMA 아키텍처
 - 분산 훈련
 - 경량화 AI
 - 오픈소스 프로젝트
aliases:
 - TinyLlama
 - TinyLlama-1.1B
 - 소형 언어 모델
url: https://github.com/jzhang38/TinyLlama
---
TinyLlama은 16개의 A100-40G GPU를 사용해 3조 토큰으로 1.1B 파라미터 LLaMA 모델을 사전 훈련하는 프로젝트입니다. LLaMA 2와 동일한 아키텍처를 채택해 호환성을 유지하면서, 제한된 계산 자원에서 효율적인 배포가 가능하도록 설계되었습니다. 에지 장치 배포, 대형 모델의 추론 가속화, 실시간 대화 생성 등 다양한 응용 사례를 지원하며, 훈련 최적화를 통해 높은 연산 효율을 달성했습니다.

**요약**:  
TinyLlama은 3조 토큰으로 훈련되는 1.1B 파라미터 소형 언어 모델로, LLaMA 2와 호환되며 에지 장치 배포에 최적화되었습니다. 16개의 A100 GPU로 90일 내 훈련이 가능하며, 훈련 과정과 체크포인트를 공개해 연구를 지원합니다. 경량화된 모델 크기와 높은 연산 효율로 실시간 응용 및 추론 가속화에 적합합니다.