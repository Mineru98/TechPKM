---
Language: Python  
tags:  
 - LLM 서빙  
 - 고성능 추론  
 - PagedAttention  
 - HuggingFace 통합  
 - 분산 추론 지원  
aliases:  
 - vLLM  
 - 페이징 어텐션  
 - LLM 서빙 라이브러리  
 - vLLM 프로젝트  
url: https://github.com/vllm-project/vllm  
---  
vLLM은 LLM 추론 및 서빙을 위한 빠르고 사용하기 쉬운 라이브러리로, UC 버클리에서 개발되어 커뮤니티로 확장되었습니다. State-of-the-art 서빙 처리량, PagedAttention 기술, 지속적인 배치 처리, 다양한 하드웨어 지원(다양한 GPU/CPU 플랫폼), 양자화 및 분산 추론을 특징으로 합니다. HuggingFace 모델과의 원활한 통합과 OpenAI 호환 API 서버를 제공하며, Llama, Mixtral, LLaVA 등 다양한 오픈소스 모델을 지원합니다.  

(참고: 요약문은 프로젝트의 핵심 목적, 기술적 특징, 사용 사례를 2~4문장으로 압축하여 작성되었습니다.)