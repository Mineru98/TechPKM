---
Language: Markdown
tags:
 - 한국어_사전학습_모델
 - NLP_모델
 - 트랜스포머_모델
aliases:
 - 한국어_PLM_목록
 - Korean_Pre-trained_Language_Models
 - 한국어_사전학습_모델_정리
url: https://github.com/sooftware/Korean-PLM/blob/main/README.md
---
이 프로젝트는 한국어 전용 사전학습 모델(PLM)을 체계적으로 정리한 레포지토리입니다. BERT, GPT, Seq2seq 계열의 모델을 각각 소형(Small), 기본(Base), 대형(Large) 사이즈로 구분하여 제공하며, 한국어 텍스트 처리에 최적화된 다양한 트랜스포머 기반 모델들을 포함합니다. 주요 적용 분야는 텍스트 분류, 질의 응답, 기계 번역 등이며, 모델별 허깅페이스 링크와 기술 스택 정보를 제공합니다.