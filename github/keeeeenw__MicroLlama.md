---
Language: Python
tags:
 - 소형 언어 모델
 - 오픈소스 LLM
 - 파인튜닝
 - 슬림파자마 데이터셋
 - 라마 아키텍처
aliases:
 - MicroLlama
 - 소형 라마 모델
 - 300M 파라미터 LLM
url: https://github.com/keeeeenw/MicroLlama
---
이 프로젝트는 300M 파라미터 규모의 소형 라마(Llama) 기반 언어 모델을 처음부터 훈련시키는 것을 목표로 합니다. 제한된 예산(500달러) 내에서 오픈소스 데이터셋인 슬림파자마(Slimpajama)를 활용해 모델을 사전 훈련시켰으며, 코딩 기능은 제한적이지만 다양한 자연어 처리 작업에 적합한 베이스 모델로 활용될 수 있습니다. TinyLlama 프로젝트를 기반으로 하여 데이터 처리 및 훈련 효율성을 개선했으며, 현재 50B 토큰 훈련된 체크포인트를 공개하고 있습니다. 평가 결과 BERT-large와 유사한 성능을 보이며 파인튜닝을 통한 활용 가능성이 높습니다.