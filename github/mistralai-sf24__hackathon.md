---
Language: Python  
tags:  
 - 허깅페이스 트랜스포머  
 - vLLM  
 - LoRA 파인튜닝  
 - 대규모 언어 모델  
 - AI 배포  
aliases:  
 - Mistral-7B  
 - 미스트랄 AI 모델  
 - Mistral 트랜스포머  
url: https://github.com/mistralai-sf24/hackathon  
---
이 프로젝트는 70억 개의 파라미터를 가진 미스트랄 대형 언어 모델의 실행 및 파인튜닝을 위한 최소 코드를 제공합니다. 모델 다운로드, 대화형 실행, vLLM 기반 배포, LoRA를 활용한 메모리 효율적인 파인튜닝 기능을 포함하며, 사전 학습 및 대화형 데이터 형식 지원이 핵심입니다.  

- **주요 특징**: 단일 GPU/멀티 GPU 학습 지원, 로그 확률 검증, Docker 기반 배포 파이프라인  
- **사용 사례**: 연구 목적의 빠른 모델 테스트 및 커스터마이징  
- **기술 스택**: PyTorch, Hugging Face Transformers, vLLM, Docker