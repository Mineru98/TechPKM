---
Language: Python
tags:
 - 모델 병합
 - Gemma-7B
 - 다중 언어 처리
 - attention 메커니즘
 - 한국어/영어
aliases:
 - Gemma Self-Attention Merger
 - 모델 어텐션 병합
 - 이중 언어 모델 생성
url: https://github.com/yourusername/gemma-self-attention-merger
---
이 프로젝트는 영어 기반과 한국어 기반의 두 Gemma 7B 모델(각각 7B)의 셀프 어텐션 레이어를 병합하여 더 큰 용량의 멀티링구얼 모델을 생성하는 것을 목표로 합니다. 어텐션 헤드를 두 배로 확장함으로써 영어와 한국어 모두에서 향상된 성능을 발휘할 수 있는 언어 모델을 구축합니다. 주요 기능은 두 모델의 어텐션 레이어 통합, 어텐션 헤드 수 확장, 다국어 처리 능력 향상입니다.